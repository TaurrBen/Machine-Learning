#  1.普通最小二乘法（Ordinary Least Square）

## 1.1 一维线性模型

​    已知一组观测数据![(x_i,y_i)](https://latex.csdn.net/eq?%28x_i%2Cy_i%29)，通过拟合模型![y = kx + b](https://latex.csdn.net/eq?y%20%3D%20kx%20&plus;%20b)。

​    理论上，两组数据即可求出确定解；但是实际情况下，观测存在误差，我们会得到多个观测数据![(x_1, y_1) , ... , (x_n, y_n)](https://latex.csdn.net/eq?%28x_1%2C%20y_1%29%20%2C%20...%20%2C%20%28x_n%2C%20y_n%29)，这时候无法得到确定解。此时我们通过使观测点尽可能多的拟合到模型，达到最佳拟合。当误差的平方和最小（即最小二乘准则）时，确实是最优。

​    **目标函数：**![Q = \underset{k,b}{min}\sum_{i=0}^{n} (y_i - \hat{y_i})^2](https://latex.csdn.net/eq?Q%20%3D%20%5Cunderset%7Bk%2Cb%7D%7Bmin%7D%5Csum_%7Bi%3D0%7D%5E%7Bn%7D%20%28y_i%20-%20%5Chat%7By_i%7D%29%5E2)

​    **求解：**![\frac{\partial Q}{\partial k} = 0](https://latex.csdn.net/eq?%5Cfrac%7B%5Cpartial%20Q%7D%7B%5Cpartial%20k%7D%20%3D%200) ，![\frac{\partial Q}{\partial b} = 0](https://latex.csdn.net/eq?%5Cfrac%7B%5Cpartial%20Q%7D%7B%5Cpartial%20b%7D%20%3D%200) 

​    容易求得：![k = \frac{ n\sum x_i y_i - \sum x_i \sum y_i}{n\sum x_i^2 - (\sum x_i)^2}](https://latex.csdn.net/eq?k%20%3D%20%5Cfrac%7B%20n%5Csum%20x_i%20y_i%20-%20%5Csum%20x_i%20%5Csum%20y_i%7D%7Bn%5Csum%20x_i%5E2%20-%20%28%5Csum%20x_i%29%5E2%7D)，![b = \frac{\sum x_i^2 \sum y_i - \sum x_i \sum x_i y_i}{n\sum x_i^2 - (\sum x_i)^2}](https://latex.csdn.net/eq?b%20%3D%20%5Cfrac%7B%5Csum%20x_i%5E2%20%5Csum%20y_i%20-%20%5Csum%20x_i%20%5Csum%20x_i%20y_i%7D%7Bn%5Csum%20x_i%5E2%20-%20%28%5Csum%20x_i%29%5E2%7D)

## 1.2 多维线性模型

​    推广到多维向量![(x_1,x_2,...,x_p)](https://latex.csdn.net/eq?%28x_1%2Cx_2%2C...%2Cx_p%29)，拟合模型![y = w_0 + w_1 x_1 + w_2 x_2 + ... + w_p x_p](https://latex.csdn.net/eq?y%20%3D%20w_0%20&plus;%20w_1%20x_1%20&plus;%20w_2%20x_2%20&plus;%20...%20&plus;%20w_p%20x_p)

​    则对于m个样本来说，可用如下线性方程组表示：

​    ![\left\{\begin{matrix} y_1 = w_0 + w_1 x_{11} + w_2 x_{12} + ... + w_p x_{1p} \\ y_2 = w_0 + w_1 x_{21} + w_2 x_{22} + ... + w_p x_{2p} \\ ... \\ y_m = w_0 + w_1 x_{m1} + w_2 x_{m2} + ... + w_p x_{mp} \end{matrix}\right.](https://latex.csdn.net/eq?%5Cleft%5C%7B%5Cbegin%7Bmatrix%7D%20y_1%20%3D%20w_0%20&plus;%20w_1%20x_%7B11%7D%20&plus;%20w_2%20x_%7B12%7D%20&plus;%20...%20&plus;%20w_p%20x_%7B1p%7D%20%5C%5C%20y_2%20%3D%20w_0%20&plus;%20w_1%20x_%7B21%7D%20&plus;%20w_2%20x_%7B22%7D%20&plus;%20...%20&plus;%20w_p%20x_%7B2p%7D%20%5C%5C%20...%20%5C%5C%20y_m%20%3D%20w_0%20&plus;%20w_1%20x_%7Bm1%7D%20&plus;%20w_2%20x_%7Bm2%7D%20&plus;%20...%20&plus;%20w_p%20x_%7Bmp%7D%20%5Cend%7Bmatrix%7D%5Cright.)

​     ![\begin{pmatrix} 1 & x_{11} & x_{12} & x_{13} & ... & x_{1p}\\ 1 & x_{21} & x_{22} & x_{23} & ... & x_{2p}\\ ... & ... & ... & ... & ... & ...\\ 1 & x_{m1} & x_{m2} & x_{m3} & ... & x_{mp} \end{pmatrix}\begin{pmatrix} w_0\\ w_1\\ ...\\ w_p \end{pmatrix} = \begin{pmatrix} y_0\\ y_1\\ ...\\ y_m \end{pmatrix}](https://latex.csdn.net/eq?%5Cbegin%7Bpmatrix%7D%201%20%26%20x_%7B11%7D%20%26%20x_%7B12%7D%20%26%20x_%7B13%7D%20%26%20...%20%26%20x_%7B1p%7D%5C%5C%201%20%26%20x_%7B21%7D%20%26%20x_%7B22%7D%20%26%20x_%7B23%7D%20%26%20...%20%26%20x_%7B2p%7D%5C%5C%20...%20%26%20...%20%26%20...%20%26%20...%20%26%20...%20%26%20...%5C%5C%201%20%26%20x_%7Bm1%7D%20%26%20x_%7Bm2%7D%20%26%20x_%7Bm3%7D%20%26%20...%20%26%20x_%7Bmp%7D%20%5Cend%7Bpmatrix%7D%5Cbegin%7Bpmatrix%7D%20w_0%5C%5C%20w_1%5C%5C%20...%5C%5C%20w_p%20%5Cend%7Bpmatrix%7D%20%3D%20%5Cbegin%7Bpmatrix%7D%20y_0%5C%5C%20y_1%5C%5C%20...%5C%5C%20y_m%20%5Cend%7Bpmatrix%7D)

​    记作![Xw = Y](https://latex.csdn.net/eq?Xw%20%3D%20Y)

​    **目标函数：**

​    ![Q = \underset{w}{min}\left \| Xw - y \right \|_2^2 \\ = \underset{w}{min}(Xw - y)^T(Xw - y) \\ = \underset{w}{min}(w^T X^T - y^T)(Xw - y) \\ = \underset{w}{min}(w^T X^T Xw - w^T X^T y - y^T Xw + y^T y) \\ = \underset{w}{min}(w^T X^T Xw - 2w^T X^T y + y^T y)](https://latex.csdn.net/eq?Q%20%3D%20%5Cunderset%7Bw%7D%7Bmin%7D%5Cleft%20%5C%7C%20Xw%20-%20y%20%5Cright%20%5C%7C_2%5E2%20%5C%5C%20%3D%20%5Cunderset%7Bw%7D%7Bmin%7D%28Xw%20-%20y%29%5ET%28Xw%20-%20y%29%20%5C%5C%20%3D%20%5Cunderset%7Bw%7D%7Bmin%7D%28w%5ET%20X%5ET%20-%20y%5ET%29%28Xw%20-%20y%29%20%5C%5C%20%3D%20%5Cunderset%7Bw%7D%7Bmin%7D%28w%5ET%20X%5ET%20Xw%20-%20w%5ET%20X%5ET%20y%20-%20y%5ET%20Xw%20&plus;%20y%5ET%20y%29%20%5C%5C%20%3D%20%5Cunderset%7Bw%7D%7Bmin%7D%28w%5ET%20X%5ET%20Xw%20-%202w%5ET%20X%5ET%20y%20&plus;%20y%5ET%20y%29)

​    **求解：**

​    ![\frac{\partial Q}{\partial w} = \frac{\partial (w^T X^T Xw - 2w^T X^T y + y^T y)}{\partial w}\\ = \frac{\partial (w^T X^T Xw - 2w^T X^T y)}{\partial w}\\ =\frac{\partial (w^T X^T Xw)}{\partial w} - 2 X^Ty\\ = \frac{\partial (w^T X^T )}{\partial w}Xw + \frac{\partial ((Xw)^T)}{\partial w}(w^T X^T )^T - 2 X^Ty\\ = 2X^T Xw - 2 X^T y](https://latex.csdn.net/eq?%5Cfrac%7B%5Cpartial%20Q%7D%7B%5Cpartial%20w%7D%20%3D%20%5Cfrac%7B%5Cpartial%20%28w%5ET%20X%5ET%20Xw%20-%202w%5ET%20X%5ET%20y%20&plus;%20y%5ET%20y%29%7D%7B%5Cpartial%20w%7D%5C%5C%20%3D%20%5Cfrac%7B%5Cpartial%20%28w%5ET%20X%5ET%20Xw%20-%202w%5ET%20X%5ET%20y%29%7D%7B%5Cpartial%20w%7D%5C%5C%20%3D%5Cfrac%7B%5Cpartial%20%28w%5ET%20X%5ET%20Xw%29%7D%7B%5Cpartial%20w%7D%20-%202%20X%5ETy%5C%5C%20%3D%20%5Cfrac%7B%5Cpartial%20%28w%5ET%20X%5ET%20%29%7D%7B%5Cpartial%20w%7DXw%20&plus;%20%5Cfrac%7B%5Cpartial%20%28%28Xw%29%5ET%29%7D%7B%5Cpartial%20w%7D%28w%5ET%20X%5ET%20%29%5ET%20-%202%20X%5ETy%5C%5C%20%3D%202X%5ET%20Xw%20-%202%20X%5ET%20y)

​    另外二阶导数即黑塞矩阵，其为![2X^TX](https://latex.csdn.net/eq?2X%5ETX)。![X^TX](https://latex.csdn.net/eq?X%5ETX)为半正定矩阵，特别是当X为列满秩时，![X^TX](https://latex.csdn.net/eq?X%5ETX)为正定矩阵，也就是意味着Q为凸函数，不会出现鞍点问题，即局部最优解是全局最优解，故可以用迭代法求出数值解。

​    **解析解（正规方程）：**

​    ![2X^T Xw - 2 X^T y = 0](https://latex.csdn.net/eq?2X%5ET%20Xw%20-%202%20X%5ET%20y%20%3D%200)

​    ①![X](https://latex.csdn.net/eq?X)列满秩：此时![X^TX](https://latex.csdn.net/eq?X%5ETX)可逆，![w = (X^T X)^{-1} X^T y](https://latex.csdn.net/eq?w%20%3D%20%28X%5ET%20X%29%5E%7B-1%7D%20X%5ET%20y)。**注**![(X^T X)^{-1} X^T](https://latex.csdn.net/eq?%28X%5ET%20X%29%5E%7B-1%7D%20X%5ET)称为![X](https://latex.csdn.net/eq?X)的左逆，![X(X^T X)^{-1} X^T](https://latex.csdn.net/eq?X%28X%5ET%20X%29%5E%7B-1%7D%20X%5ET)为投影矩阵，其作用是向矩阵![X](https://latex.csdn.net/eq?X)列空间做正交投影。

​    ②![X](https://latex.csdn.net/eq?X)不满秩：此时![w = X^+ y](https://latex.csdn.net/eq?w%20%3D%20X%5E&plus;%20y)。**注**对![X](https://latex.csdn.net/eq?X)进行奇异值分解![X = U \Sigma V^{-1}](https://latex.csdn.net/eq?X%20%3D%20U%20%5CSigma%20V%5E%7B-1%7D)，则伪逆![X^+ = V \Sigma^{-1} U^{-1}](https://latex.csdn.net/eq?X%5E&plus;%20%3D%20V%20%5CSigma%5E%7B-1%7D%20U%5E%7B-1%7D)，此时![X X^+= U \Sigma V^{-1} V \Sigma^{-1} U^{-1} = U \Sigma \Sigma^{-1} U^{-1} = U \Sigma \Sigma^{-1} U^{T}](https://latex.csdn.net/eq?X%20X%5E&plus;%3D%20U%20%5CSigma%20V%5E%7B-1%7D%20V%20%5CSigma%5E%7B-1%7D%20U%5E%7B-1%7D%20%3D%20U%20%5CSigma%20%5CSigma%5E%7B-1%7D%20U%5E%7B-1%7D%20%3D%20U%20%5CSigma%20%5CSigma%5E%7B-1%7D%20U%5E%7BT%7D)，![U](https://latex.csdn.net/eq?U)是![X](https://latex.csdn.net/eq?X)列空间的一组标准正交基，![X X^+](https://latex.csdn.net/eq?X%20X%5E&plus;)是列空间的投影矩阵。故可以借助伪逆![X^+](https://latex.csdn.net/eq?X%5E&plus;)得到一个解![w = X^+ y](https://latex.csdn.net/eq?w%20%3D%20X%5E&plus;%20y)

​    **数值解：**

​    **①**梯度下降法**：**![w_{k+1} = w_{k} - \alpha \frac{\partial Q}{\partial w} =w_{k} - \alpha(2X^T Xw - 2 X^T y)](https://latex.csdn.net/eq?w_%7Bk&plus;1%7D%20%3D%20w_%7Bk%7D%20-%20%5Calpha%20%5Cfrac%7B%5Cpartial%20Q%7D%7B%5Cpartial%20w%7D%20%3Dw_%7Bk%7D%20-%20%5Calpha%282X%5ET%20Xw%20-%202%20X%5ET%20y%29)

​    ②随机梯度下降法：

​    **复杂度：**使用![X](https://latex.csdn.net/eq?X)的奇异值分解来计算最小二乘解。如果![X](https://latex.csdn.net/eq?X)是一个形状为![(n_{samples}, n_{features})](https://latex.csdn.net/eq?%28n_%7Bsamples%7D%2C%20n_%7Bfeatures%7D%29)的矩阵，设![n_{samples} \geq n_{features}](https://latex.csdn.net/eq?n_%7Bsamples%7D%20%5Cgeq%20n_%7Bfeatures%7D)，则该方法的复杂度为![O(n_{samples} n_{fearures}^2)](https://latex.csdn.net/eq?O%28n_%7Bsamples%7D%20n_%7Bfearures%7D%5E2%29)

## 1.3 最小二乘准则

​    **概率统计视角：**

​    假定每个样本的损失或者说残差为
 独立同分布，且服从![N(0, \sigma^2)](https://latex.csdn.net/eq?N%280%2C%20%5Csigma%5E2%29)，则似然函数为：    ​​

​    ![L = \prod_{i = 1}^{N} \frac{1}{\sqrt {2\pi}}exp[-\frac{1}{2}(\frac{y_i - \hat{y_i}}{\sigma})^2]](https://latex.csdn.net/eq?L%20%3D%20%5Cprod_%7Bi%20%3D%201%7D%5E%7BN%7D%20%5Cfrac%7B1%7D%7B%5Csqrt%20%7B2%5Cpi%7D%7Dexp%5B-%5Cfrac%7B1%7D%7B2%7D%28%5Cfrac%7By_i%20-%20%5Chat%7By_i%7D%7D%7B%5Csigma%7D%29%5E2%5D)

​    取对数可得到对数似然函数为：

​    ![ln L \\= Nln(\frac{1}{\sqrt {2\pi}}) + \sum_{i=1}^{N}[-\frac{1}{2}(\frac{y_i - \hat{y_i}}{\sigma})^2] \\ = -Nln\sqrt{2\pi} - \frac{1}{2\sigma^2}\sum_{i=1}^{N}({y_i - \hat{y_i}})^2](https://latex.csdn.net/eq?ln%20L%20%5C%5C%3D%20Nln%28%5Cfrac%7B1%7D%7B%5Csqrt%20%7B2%5Cpi%7D%7D%29%20&plus;%20%5Csum_%7Bi%3D1%7D%5E%7BN%7D%5B-%5Cfrac%7B1%7D%7B2%7D%28%5Cfrac%7By_i%20-%20%5Chat%7By_i%7D%7D%7B%5Csigma%7D%29%5E2%5D%20%5C%5C%20%3D%20-Nln%5Csqrt%7B2%5Cpi%7D%20-%20%5Cfrac%7B1%7D%7B2%5Csigma%5E2%7D%5Csum_%7Bi%3D1%7D%5E%7BN%7D%28%7By_i%20-%20%5Chat%7By_i%7D%7D%29%5E2)

​    故可知，最大化上述对数似然函数等价于最小化 ![\sum_{i=1}^{N}({y_i - \hat{y_i}})^2](https://latex.csdn.net/eq?%5Csum_%7Bi%3D1%7D%5E%7BN%7D%28%7By_i%20-%20%5Chat%7By_i%7D%7D%29%5E2)，这也被称为最小二乘准则。这表明了线性回归实际上假定了数据是服从正态分布。

## 1.4 Python实现（暂仅包含调库）

```python
from sklearn import linear_model
reg = linear_model.LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1)
reg.fit ([[0, 0], [1, 1], [2, 2]], [0, 1, 2])
print(reg.coef_,reg.intercept_) #公式中的w
```

![点击并拖拽以移动](data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==)

## 1.5 拓展

​    **案例1：普通最小二乘法**

```python
# Authors: The scikit-learn developers
# SPDX-License-Identifier: BSD-3-Clause

# Data Loading and Preparation
from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split

X, y = load_diabetes(return_X_y=True)
X = X[:, [2]]  # Use only one feature
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=20, shuffle=False)

# Linear regression model
from sklearn.linear_model import LinearRegression

regressor = LinearRegression().fit(X_train, y_train)

# Model evaluation
from sklearn.metrics import mean_squared_error, r2_score

y_pred = regressor.predict(X_test)

print(f"Mean squared error: {mean_squared_error(y_test, y_pred):.2f}")
print(f"Coefficient of determination: {r2_score(y_test, y_pred):.2f}")

# Plotting the results
import matplotlib.pyplot as plt

fig, ax = plt.subplots(ncols=2, figsize=(10, 5), sharex=True, sharey=True)

ax[0].scatter(X_train, y_train, label="Train data points")
ax[0].plot(
    X_train,
    regressor.predict(X_train),
    linewidth=3,
    color="tab:orange",
    label="Model predictions",
)
ax[0].set(xlabel="Feature", ylabel="Target", title="Train set")
ax[0].legend()

ax[1].scatter(X_test, y_test, label="Test data points")
ax[1].plot(X_test, y_pred, linewidth=3, color="tab:orange", label="Model predictions")
ax[1].set(xlabel="Feature", ylabel="Target", title="Test set")
ax[1].legend()

fig.suptitle("Linear Regression")

plt.show()
```

![点击并拖拽以移动](data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==)

​    **案例2：正约束线性回归模型**

```python
# Authors: The scikit-learn developers
# SPDX-License-Identifier: BSD-3-Clause

import matplotlib.pyplot as plt
import numpy as np

from sklearn.metrics import r2_score

# Generate some random data
np.random.seed(42)

n_samples, n_features = 200, 50
X = np.random.randn(n_samples, n_features)
true_coef = 3 * np.random.randn(n_features)
# Threshold coefficients to render them non-negative
true_coef[true_coef < 0] = 0
y = np.dot(X, true_coef)

# Add some noise
y += 5 * np.random.normal(size=(n_samples,))

# Split the data in train set and test set
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)

# Fit the Non-Negative least squares.
from sklearn.linear_model import LinearRegression

reg_nnls = LinearRegression(positive=True)
y_pred_nnls = reg_nnls.fit(X_train, y_train).predict(X_test)
r2_score_nnls = r2_score(y_test, y_pred_nnls)
print("NNLS R2 score", r2_score_nnls)

# Fit an OLS.
reg_ols = LinearRegression()
y_pred_ols = reg_ols.fit(X_train, y_train).predict(X_test)
r2_score_ols = r2_score(y_test, y_pred_ols)
print("OLS R2 score", r2_score_ols)

# Comparing the regression coefficients between OLS and NNLS
fig, ax = plt.subplots()
ax.plot(reg_ols.coef_, reg_nnls.coef_, linewidth=0, marker=".")

low_x, high_x = ax.get_xlim()
low_y, high_y = ax.get_ylim()
low = max(low_x, low_y)
high = min(high_x, high_y)
ax.plot([low, high], [low, high], ls="--", c=".3", alpha=0.5)
ax.set_xlabel("OLS regression coefficients", fontweight="bold")
ax.set_ylabel("NNLS regression coefficients", fontweight="bold")
```

![点击并拖拽以移动](data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==)

​    **案例3：多项式拟合非线性函数（推导见参考文章）**

```python
'''
最小二乘法拟合函数曲线f(x)
1、拟合多项式为：y = a0 + a1*x + a2*x^2 + ... + ak*x^k
2、求每个点到曲线的距离之和：Loss = ∑(yi - (a0 + a1*x + a2*x^2 + ... + ak*x^k))^2
3、最优化Loss函数，即求Loss对a0,a1,...ak的偏导数为0
    3.1、数学解法——求解线性方程组：
        整理最优化的偏导数矩阵为：X：含有xi的系数矩阵，A：含有ai的系数矩阵，Y：含有yi的系数矩阵
        求解：XA=Y中的A矩阵
    3.2、迭代解法——梯度下降法：
        计算每个系数矩阵A[k]的梯度，并迭代更新A[k]的梯度
        A[k] = A[k] - (learn_rate * gradient)
'''
import numpy as np
import matplotlib.pyplot as plt
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False
'''
高斯列主消元算法
'''
# 得到增广矩阵
def get_augmented_matrix(matrix, b):
    row, col = np.shape(matrix)
    matrix = np.insert(matrix, col, values=b, axis=1)
    return matrix
# 取出增广矩阵的系数矩阵（第一列到倒数第二列）
def get_matrix(a_matrix):
    return a_matrix[:, :a_matrix.shape[1] - 1]
# 选列主元，在第k行后的矩阵里，找出最大值和其对应的行号和列号
def get_pos_j_max(matrix, k):
    max_v = np.max(matrix[k:, :])
    pos = np.argwhere(matrix == max_v)
    i, _ = pos[0]
    return i, max_v
# 矩阵的第k行后，行交换
def exchange_row(matrix, r1, r2, k):
    matrix[[r1, r2], k:] = matrix[[r2, r1], k:]
    return matrix
# 消元计算(初等变化)
def elimination(matrix, k):
    row, col = np.shape(matrix)
    for i in range(k + 1, row):
        m_ik = matrix[i][k] / matrix[k][k]
        matrix[i] = -m_ik * matrix[k] + matrix[i]
    return matrix
# 回代求解
def backToSolve(a_matrix):
    matrix = a_matrix[:, :a_matrix.shape[1] - 1]  # 得到系数矩阵
    b_matrix = a_matrix[:, -1]  # 得到值矩阵
    row, col = np.shape(matrix)
    x = [None] * col  # 待求解空间X
    # 先计算上三角矩阵对应的最后一个分量
    x[-1] = b_matrix[col - 1] / matrix[col - 1][col - 1]
    # 从倒数第二行开始回代x分量
    for _ in range(col - 1, 0, -1):
        i = _ - 1
        sij = 0
        xidx = len(x) - 1
        for j in range(col - 1, i, -1):
            sij += matrix[i][j] * x[xidx]
            xidx -= 1
        x[xidx] = (b_matrix[i] - sij) / matrix[i][i]
    return x
# 求解非齐次线性方程组：ax=b
def solve_NLQ(a, b):
    a_matrix = get_augmented_matrix(a, b)
    for k in range(len(a_matrix) - 1):
        # 选列主元
        max_i, max_v = get_pos_j_max(get_matrix(a_matrix), k=k)
        # 如果A[ik][k]=0，则矩阵奇异退出
        if a_matrix[max_i][k] == 0:
            print('矩阵A奇异')
            return None, []
        if max_i != k:
            a_matrix = exchange_row(a_matrix, k, max_i, k=k)
        # 消元计算
        a_matrix = elimination(a_matrix, k=k)
    # 回代求解
    X = backToSolve(a_matrix)
    return a_matrix, X
'''
最小二乘法多项式拟合曲线
'''
# 生成带有噪点的待拟合的数据集合
def init_fx_data():
    # 待拟合曲线f(x) = sin2x * [(x^2 - 1)^3 + 0.5]
    xs = np.arange(-1, 1, 0.01)  # 200个点
    ys = [((x ** 2 - 1) ** 3 + 0.5) * np.sin(x * 2) for x in xs]
    ys1 = []
    for i in range(len(ys)):
        z = np.random.randint(low=-10, high=10) / 100  # 加入噪点
        ys1.append(ys[i] + z)
    return xs, ys1
# 计算最小二乘法当前的误差
def last_square_current_loss(xs, ys, A):
    error = 0.0
    for i in range(len(xs)):
        y1 = 0.0
        for k in range(len(A)):
            y1 += A[k] * xs[i] ** k
        error += (ys[i] - y1) ** 2
    return error
# 迭代解法：最小二乘法+梯度下降法
def last_square_fit_curve_Gradient(xs, ys, order, iternum=1000, learn_rate=0.001):
    A = [0.0] * (order + 1)
    for r in range(iternum + 1):
        for k in range(len(A)):
            gradient = 0.0
            for i in range(len(xs)):
                y1 = 0.0
                for j in range(len(A)):
                    y1 += A[j] * xs[i]**j
                gradient += -2 * (ys[i] - y1) * xs[i]**k  # 计算A[k]的梯度
            A[k] = A[k] - (learn_rate * gradient)  # 更新A[k]的梯度
        # 检查误差变化
        if r % 100 == 0:
            error = last_square_current_loss(xs=xs, ys=ys, A=A)
            print('最小二乘法+梯度下降法：第{}次迭代，误差下降为：{}'.format(r, error))
    return A
# 数学解法：最小二乘法+求解线性方程组
def last_square_fit_curve_Gauss(xs, ys, order):
    X, Y = [], []
    # 求解偏导数矩阵里，含有xi的系数矩阵X
    for i in range(0, order + 1):
        X_line = []
        for j in range(0, order + 1):
            sum_xi = 0.0
            for xi in xs:
                sum_xi += xi ** (j + i)
            X_line.append(sum_xi)
        X.append(X_line)
    # 求解偏导数矩阵里，含有yi的系数矩阵Y
    for i in range(0, order + 1):
        Y_line = 0.0
        for j in range(0, order + 1):
            sum_xi_yi = 0.0
            for k in range(len(xs)):
                sum_xi_yi += (xs[k] ** i * ys[k])
            Y_line = sum_xi_yi
        Y.append(Y_line)
    a_matrix, A = solve_NLQ(np.array(X), Y)  # 高斯消元：求解XA=Y的A
    # A = np.linalg.solve(np.array(X), np.array(Y))  # numpy API 求解XA=Y的A
    error = last_square_current_loss(xs=xs, ys=ys, A=A)
    print('最小二乘法+求解线性方程组，误差下降为：{}'.format(error))
    return A
# 可视化多项式曲线拟合结果
def draw_fit_curve(xs, ys, A, order):
    fig = plt.figure()
    ax = fig.add_subplot(111)
    fit_xs, fit_ys = np.arange(min(xs) * 0.8, max(xs) * 0.8, 0.01), []
    for i in range(0, len(fit_xs)):
        y = 0.0
        for k in range(0, order + 1):
            y += (A[k] * fit_xs[i] ** k)
        fit_ys.append(y)
    ax.plot(fit_xs, fit_ys, color='g', linestyle='-', marker='', label='多项式拟合曲线')
    ax.plot(xs, ys, color='m', linestyle='', marker='.', label='曲线真实数据')
    plt.title(s='最小二乘法拟合多项式N={}的函数曲线f(x)'.format(order))
    plt.legend()
    plt.show()
if __name__ == '__main__':
    order = 10  # 拟合的多项式项数
    xs, ys = init_fx_data()  # 曲线数据
    # 数学解法：最小二乘法+求解线性方程组
    A = last_square_fit_curve_Gauss(xs=xs, ys=ys, order=order)
    # 迭代解法：最小二乘法+梯度下降
    # A = last_square_fit_curve_Gradient(xs=xs, ys=ys, order=order, iternum=10000, learn_rate=0.001)
    draw_fit_curve(xs=xs, ys=ys, A=A, order=order)  # 可视化多项式曲线拟合结果
```

![点击并拖拽以移动](data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==)

## 1.6 Tricks

​    **最小二乘法的限制：**要保证最小二乘法有解，就得保证![X^TX](https://latex.csdn.net/eq?X%5ETX)可逆，故需要探究什么情况下不可逆，不可逆又该怎么办？

​    ①当样本的数量小于参数向量![w](https://latex.csdn.net/eq?w)时，![X^TX](https://latex.csdn.net/eq?X%5ETX)不可逆，此时解决办法是增加样本数量；

​    ②![X](https://latex.csdn.net/eq?X)中的某个特征与其他特征线性相关时，![X^TX](https://latex.csdn.net/eq?X%5ETX)不可逆，可通过筛选线性无关的特征解决；

​    ③加入正则化的方法，常见的有L1正则项和L2正则项。另见文章

## 1.7 Reference

①一文掌握线性回归的基本理论 https://zhuanlan.zhihu.com/p/590738723

②最小二乘法—多项式拟合非线性函数https://blog.csdn.net/boonya/article/details/95336464

④sklearn官方指南https://scikit-learn.org/stable/user_guide.html

#  2. 岭回归和分类（Ridge regression and classification）

## 2.1. 岭回归

​    岭回归实际上是一种通过对系数大小施加惩罚（正则化项）以解决***\*[普通最小二乘法](https://blog.csdn.net/2404_88356174/article/details/152451784?spm=1001.2014.3001.5502)\****的多重共线性问题的诱有偏估计方法，放弃了最小二乘法的无偏性，以降低精度为代价来防止过拟合。以下根据***\*[普通最小二乘法](https://blog.csdn.net/2404_88356174/article/details/152451784?spm=1001.2014.3001.5502)\****并且引入L2正则化项来拟合多维线性模型。

​    **目标函数：**

​    ![Q = \underset{w}{min}\left \| Xw - y \right \|_2^2 + \lambda \left \| w \right \|_2^2 \\ = \underset{w}{min}(w^T X^T Xw - 2w^T X^T y + y^T y + \lambda w^T w )](https://latex.csdn.net/eq?Q%20%3D%20%5Cunderset%7Bw%7D%7Bmin%7D%5Cleft%20%5C%7C%20Xw%20-%20y%20%5Cright%20%5C%7C_2%5E2%20&plus;%20%5Clambda%20%5Cleft%20%5C%7C%20w%20%5Cright%20%5C%7C_2%5E2%20%5C%5C%20%3D%20%5Cunderset%7Bw%7D%7Bmin%7D%28w%5ET%20X%5ET%20Xw%20-%202w%5ET%20X%5ET%20y%20&plus;%20y%5ET%20y%20&plus;%20%5Clambda%20w%5ET%20w%20%29)

​    **求解： ![\frac{\partial Q}{\partial w} = 2X^T Xw - 2 X^T y + 2\lambda w](https://latex.csdn.net/eq?%5Cfrac%7B%5Cpartial%20Q%7D%7B%5Cpartial%20w%7D%20%3D%202X%5ET%20Xw%20-%202%20X%5ET%20y%20+%202%5Clambda%20w)**

​    **解析解： ![2X^T Xw - 2 X^T y + 2\lambda w = 0\Rightarrow w = (X^T X + \lambda E)^{-1}X^T y](https://latex.csdn.net/eq?2X%5ET%20Xw%20-%202%20X%5ET%20y%20+%202%5Clambda%20w%20%3D%200%5CRightarrow%20w%20%3D%20%28X%5ET%20X%20+%20%5Clambda%20E%29%5E%7B-1%7DX%5ET%20y)**

​    **注：**此处引入正则化项，则![X^T X + \lambda E](https://latex.csdn.net/eq?X%5ET%20X%20&plus;%20%5Clambda%20E)一定是正定矩阵，确保了其可逆性（见线性代数理论）。我们通常称 ![\lambda](https://latex.csdn.net/eq?%5Clambda) 为岭系数，且 ![\lambda \geq 0](https://latex.csdn.net/eq?%5Clambda%20%5Cgeq%200) ，①当![\lambda = 0](https://latex.csdn.net/eq?%5Clambda%20%3D%200)时，得到最小二乘解；②当 ![\lambda](https://latex.csdn.net/eq?%5Clambda) 越大，对于![w](https://latex.csdn.net/eq?w)的估计越趋向于0；

## **2.2. 岭分类**

​    岭分类是岭回归的一个分类器变体。对于二分类任务，通常先将二元目标转换为![(1,-1)](https://latex.csdn.net/eq?%281%2C-1%29)，然后视为回归任务，进行优化。而对于多类分类任务，视其为多输出回归。

## **2.3. Python实现（**暂仅包含调库**）**

​    **岭回归：**

```python
from sklearn import linear_model
reg = linear_model.Ridge (alpha=0.5, copy_X=True, fit_intercept=True, max_iter=None,
 normalize=False, random_state=None, solver='auto', tol=0.001)
reg.fit ([[0, 0], [0, 0], [1, 1]], [0, .1, 1])
print(reg.coef_,reg.intercept_)
```

![点击并拖拽以移动](data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==)

​    其中通过设置solver来选择求解器

| ‘lbfgs’    | 默认为非负情形即positive=True |
| ---------- | ----------------------------- |
| ‘cholesky’ | X矩阵不是一个稀疏矩阵         |
| sparse_cg  | 上述条件均不满足              |



​    **岭分类：见下案例3**

## **2.4. 拓展**

​    **案例1：绘制岭迹图**

```python
import matplotlib.pyplot as plt
import numpy as np

from sklearn import linear_model

# X is the 10x10 Hilbert matrix 希尔伯特矩阵
print(np.arange(0, 10))
X = 1.0 / (np.arange(1, 11) + np.arange(0, 10)[:, np.newaxis])
y = np.ones(10)

# Compute paths
n_alphas = 200
alphas = np.logspace(-10, -2, n_alphas)

coefs = []
for a in alphas:
    ridge = linear_model.Ridge(alpha=a, fit_intercept=False)
    ridge.fit(X, y)
    coefs.append(ridge.coef_)

# Display results
ax = plt.gca()

ax.plot(alphas, coefs)
ax.set_xscale("log")
ax.set_xlim(ax.get_xlim()[::-1])  # reverse axis
plt.xlabel("alpha")
plt.ylabel("weights")
plt.title("岭迹图 (alpha)")
plt.axis("tight")
plt.legend(
    [f"Feature {i + 1}" for i in range(X.shape[1])], loc="best", fontsize="small"
)
plt.show()
```

![点击并拖拽以移动](data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==)

​    **案例2：对比普通最小二乘法与岭回归****（当数据稀疏或是噪声较大时，岭回归具有更小的方差，即更加稳定）**

```python
# Authors: The scikit-learn developers
# SPDX-License-Identifier: BSD-3-Clause

import matplotlib.pyplot as plt
import numpy as np

from sklearn import linear_model

X_train = np.c_[0.5, 1].T
y_train = [0.5, 1]
X_test = np.c_[0, 2].T

np.random.seed(0)

classifiers = dict(
    ols=linear_model.LinearRegression(), ridge=linear_model.Ridge(alpha=0.1)
)

for name, clf in classifiers.items():
    fig, ax = plt.subplots(figsize=(4, 3))

    for _ in range(6):
        this_X = 0.1 * np.random.normal(size=(2, 1)) + X_train
        clf.fit(this_X, y_train)

        ax.plot(X_test, clf.predict(X_test), color="gray")
        ax.scatter(this_X, y_train, s=3, c="gray", marker="o", zorder=10)

    clf.fit(X_train, y_train)
    ax.plot(X_test, clf.predict(X_test), linewidth=2, color="blue")
    ax.scatter(X_train, y_train, s=30, c="red", marker="+", zorder=10)

    ax.set_title(name)
    ax.set_xlim(0, 2)
    ax.set_ylim((0, 1.6))
    ax.set_xlabel("X")
    ax.set_ylabel("y")

    fig.tight_layout()

plt.show()
```

![点击并拖拽以移动](data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==)

​    **案例3：使用稀疏特征对文本文档分类**

```python
# Authors: The scikit-learn developers
# SPDX-License-Identifier: BSD-3-Clause

from time import time

from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer

categories = [
    "alt.atheism",
    "talk.religion.misc",
    "comp.graphics",
    "sci.space",
]


def size_mb(docs):
    return sum(len(s.encode("utf-8")) for s in docs) / 1e6


def load_dataset(verbose=False, remove=()):
    """Load and vectorize the 20 newsgroups dataset."""

    data_train = fetch_20newsgroups(
        subset="train",
        categories=categories,
        shuffle=True,
        random_state=42,
        remove=remove,
    )

    data_test = fetch_20newsgroups(
        subset="test",
        categories=categories,
        shuffle=True,
        random_state=42,
        remove=remove,
    )

    # order of labels in `target_names` can be different from `categories`
    target_names = data_train.target_names

    # split target in a training set and a test set
    y_train, y_test = data_train.target, data_test.target

    # Extracting features from the training data using a sparse vectorizer
    t0 = time()
    vectorizer = TfidfVectorizer(
        sublinear_tf=True, max_df=0.5, min_df=5, stop_words="english"
    )
    X_train = vectorizer.fit_transform(data_train.data)
    duration_train = time() - t0

    # Extracting features from the test data using the same vectorizer
    t0 = time()
    X_test = vectorizer.transform(data_test.data)
    duration_test = time() - t0

    feature_names = vectorizer.get_feature_names_out()

    if verbose:
        # compute size of loaded data
        data_train_size_mb = size_mb(data_train.data)
        data_test_size_mb = size_mb(data_test.data)

        print(
            f"{len(data_train.data)} documents - "
            f"{data_train_size_mb:.2f}MB (training set)"
        )
        print(f"{len(data_test.data)} documents - {data_test_size_mb:.2f}MB (test set)")
        print(f"{len(target_names)} categories")
        print(
            f"vectorize training done in {duration_train:.3f}s "
            f"at {data_train_size_mb / duration_train:.3f}MB/s"
        )
        print(f"n_samples: {X_train.shape[0]}, n_features: {X_train.shape[1]}")
        print(
            f"vectorize testing done in {duration_test:.3f}s "
            f"at {data_test_size_mb / duration_test:.3f}MB/s"
        )
        print(f"n_samples: {X_test.shape[0]}, n_features: {X_test.shape[1]}")

    return X_train, X_test, y_train, y_test, feature_names, target_names

X_train, X_test, y_train, y_test, feature_names, target_names = load_dataset(verbose=True)

# model
from sklearn.linear_model import RidgeClassifier

clf = RidgeClassifier(tol=1e-2, solver="sparse_cg")
clf.fit(X_train, y_train)
pred = clf.predict(X_test)

# ConfusionMatrixDisplay
import matplotlib.pyplot as plt

from sklearn.metrics import ConfusionMatrixDisplay

fig, ax = plt.subplots(figsize=(10, 5))
ConfusionMatrixDisplay.from_predictions(y_test, pred, ax=ax)
ax.xaxis.set_ticklabels(target_names)
ax.yaxis.set_ticklabels(target_names)
_ = ax.set_title(
    f"Confusion Matrix for {clf.__class__.__name__}\non the original documents"
)

plt.show()
```

![点击并拖拽以移动](data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==)

## **2.5. Tricks**

①可以通过观察岭迹图，岭系数应选取喇叭口附近的值较为合适。

## 2.6. Reference

 ①sklearn官方指南https://scikit-learn.org/stable/user_guide.html